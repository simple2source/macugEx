# 抓取系统


## TODO

- 进程的执行信息统一到一个文件中记录, 取代或者说替换现在的 `/data/fetch/51job_successtask_51_TASK_ID.txt` 的功能, 记录信息需要包括但不限于: 抓取的号段信息, 启动时间, 最后成功抓取的时间, 最近一段时间抓取的效率指标(根据具体情况看什么指标好计算, 比如最近1个小时抓取数, id命中率), 使用登录账户的信息(登录时间/账号名/是否已过期等)
- 需要提供 web 服务(http协议方式访问, 命令行方式提供, 由 init.php 来调用也可以), 接受简历来源和 id 参数, 可以随时抓取指定的 id 并返回简历 id/电话号码(如果有)
- 可以先知考虑单机, 但是以后可能要扩充到多机: 需要有任务池, 登录用户池, 整体的抓取速度/每个用户的抓取速度等多维度可以有封顶限制
- 运行python抓取进程的时候,存在多个进程对task文件任务数进行重复抓取,[解决设想] -- 检测生成对应的pid文件进行判定


优先级| 标题           | 详细说明
-----|----------------|----
3    | 目录结构优化    | TODO
2    | 运行监控        | TODO


## fetch 使用说明

### 1. 运行fetch抓取
- 抓取 51job /data/fetch 目录下执行 ```python main.py 51 /data/spider/cookie/[对应的cookie文件] /data/fetch/task/51/[对应的task文件]```

- 通过搜索页面抓取 cjol /data/fetch 目录下执行 ```python main.py cjolsearch /data/spider/cookie/cjolsearch.txt /data/fetch/task/cjolsearch/task_zone.txt```

- ```ps -aux | grep python``` 查看运行状态

- 抓取成功标志:  ```/data/fetch/sucess[对应任务名字]```  生成对应的文件,并且里面的数字有增长,数字代表当前抓取的条数


### 2. fetch系统组成

- ```/data/spider/cookie/[对应文件名]``` 通过helper动态抓取的帐号cookier实时同步到此文件,```ls -trl``` 查看cookie文件更新时间状态

- ```/data/fetch/task/51/[对应任务名]``` 自定义任务抓取id,格式如下:1,100;1,第一位代表初始范围id进行抓取;第二位代表到此id结束;第三位代表抓取系统从此id进行抓取一路遍历下去.

- ```/data/fetch/[sucess标识计数文件]``` 当生成文件,可以实时查看获取的当前已经抓取的条目数.

- ```/data/fetch/lib51search.py``` 通过加载本地文件,实现关键字搜索页面进行抓取,需要指定关键字作为文件加载

- ```/data/fetch/main.py``` 代码运行主文件

 - ```/data/fetch/libcjolsearch.py``` 通过加载cookie 文件，循环抓取搜索页面第一页

### 3. 问题分析

-  **1.问题** . 两个帐号spxx373, spxx336抓取不到文件,一直报错"最后没有抓取对应文件."
  +  ,经排查得出如下结论:由于抓取策略是依据id拼接成链接,并保存下来,链接方式如下: http://ehire.51job.com/Candidate/ResumeView.aspx?hidUserID=336240513 ,此两帐号经测试无法通过网页的方式来访问此页面,必须通过搜索并进行后面校验来获取页面,而其他帐号却可以通过这种网页id形式来直接访问页面,证明此2个帐号权限方面存在问题,另外还有其他几个帐号存在比较低的命中率,是否与权限有关,还需要进一步确定.

### 4. 搜索的方式抓取条件 更新:2016-1-12
-  通过搜索的方式抓取简历条件为:居住地+工作年限+年龄+性别+学历(部分过滤掉了大专以下学历),抓取的条件分的情况很细,容易出现很多搜索条件出来的页面是错误的,针对这种情况,对请求的错误页面做了判断并休眠5s避免触碰到抓取过于频繁.

hidWhere=00%230%230%230%7C99%7C20151231%7C20160107%7C31%7C31%7C7%7C7%7C99%7C000000%7C000000%7C99%7C99%7C99%7C0000%7C99%7C99%7C99%7C00%7C0000%7C99%7C99%7C99%7C0000%7C99%7C99%7C00%7C99%7C99%7C99%7C99%7C99%7C99%7C99%7C99%7C99%7C020000%7C0%7C0%7C0000%7C99%23%25BeginPage%25%23%25EndPage%25%23&hidSearchNameID=&hidEhireDemo=&hidNoSearch=&hidYellowTip=0 ***  其中要找到特定的post_data才能实现请求对应页面


### 5. 注意事项

- 抓取正常代码不多, 注意处理要严谨, 对比较可能的情况作出判断
  - 抓取到的网页根据具体情况有根据字符串判断正常抓取成功的标准, 保证抓取到的是数据, 而不是登录或者其他错误提示页面, 出错了一律记录 log 并保存出错页面
  - 当抓取账号出现数量限制等异常的时候, 终止抓取并发邮件出来

- Cookie 文件的处理，因为Unix系系统默认在文件末尾添加新行，而vi又不显示新行（Vim 7.4.785可以处理），有可能导致refer失效（cjol)，所以cookie文件的修改不要用vi。


##  智联

###  1.运行说明
- ``` python main.py zhilian /data/spider/cookie/cookie_id.txt /data/fetch/task/zhilian/task_zone.txt ```
- 通过搜索获取最新简历抓取,现在任务条件都直接写在程序当中,加载任务文件路径仅是为了传递一个参数并且用来生成记录抓取成功数目

###  2.改进空间

- 目前最难的, 不能按照 id 搜索, kelvin 有说根据公司名结合期望工作地等查, 意义不大, 跟关键词搜差不多
- 技术上可以把每个简历里推荐的也提炼出来, 但是基本只能推荐最近的, 比较老的应该抓不到
- 由于每天查看的数量是20000个,达到限制后会['您今天查看的简历数量已经超过限制，请明天继续查看!'],现在依据此判断发送告警邮件,并结束程序,由于目前cookie是写死路径的,需要考虑自动重新加载cookie的方法
- 以更新的频率来看,单个程序抓取的速度有效简历大概极限为2W-3W[一般可能在1W5],并且考虑帐号限制,单个进程难以覆盖到近2天更新的简历
- 以目前抓取的的情况来分析,单个进程的条件并不能完全覆盖大部分简历数,依据多个进程将不同的条件分割抓取,可以最大限度的抓取简历,抓取简历的条件应该优先考虑必要的条件,目前抓取的条件是依据task文件做判断,执行相应的条件抓取.
- 依据搜索来抓取每次一个条件最多显示67页,条件越多简历数目越多,年龄,行业是作为参数选项目前来看是一个比较好的条件,这个范围区间比较广,数目比较多

###  3.抓取策略
- 目前抓取的是按照工作年限+期望工作地进行组合获取最近更新的简历,由于每天更新的简历大概为45000个以上[以更新的最大频率来计算],此组合条件基本可以覆盖95%以上的简历更新.[后续经过验证,发现许多是重复更新的,实际上不同的简历是没有达到45000,大概是20000或以下,可能与具体日期有关]
- 目前依据页面的日期进行判断,抓取最近2天更新的简历.
- 抓取过程中发现以最大为2s间隔抓取到一定数量后会出现要求输入验证码,并且输入验证码后,另外过几分钟又会要求输入验证码,将此增大为3s后,暂时要求输入验证码频率立即下降很多,目前将输入验证码作为警报发出来,依据当前抓取的cookie,网页上提交验证码则可以继续抓取.
- 职位与行业搭配搜索效果并不好
- 按工作年限搜索并不准确，因为搜索一年到一年加两年到两年并不能覆盖1年到两年的结果，搜索的问题，但是要注意
- 按年龄或性别搜索非常准确
- 智联的登录态可以长期维持，几天都可以用
- 智联可以搜索3天内更新的简历，这对持续搜索很有帮助
- 现在还没有测试出限制频率，不过可以知道一个小时2000个简历是没有问题的
- 智联适合用条件组合来抓取简历，已经证实不合适用简历id来搜索简历
- 智联提速可能只有在初期，利用各类条件覆盖搜索来抓取可观的数据量，后期只要抓取最近更新的简历则好
- 每天抓取的最大简历数大概2万左右, 会显示 ”系统显示：您的操作过于频繁，请注意劳逸结合，休息一会再来！“，判断有没有这句话就好

###  4.常见错误

- 一般来说出现一些未知错误，但是都是可以直接启动进程解决的
- 曾出现过进程僵死的情况，很久都没抓取（现在只能ctrl c把进程干掉再启动，出现原因未明）
- 超时错误，参考51job的超时错误
- json decode error 直接重启进程就好
- httplib.badstatus 错误 ，参考51job的httplib.badstatus 错误
- 出现状态码503的错误，初步猜测抓取触碰了一些机制，可能是抓取数量，不过再重启进程就好了，不需要做特殊处理
- sqlite3.OperationalError: database is locked 写日记的时候加锁了，只能重启进程
- ValueError: No JSON object could be decoded 重启程序就好
- httplib.IncompleteRead: IncompleteRead (这个错误的出现 HTTP通道返回0字节，或者返回一定字节后卡住) ，重启进程
- **'因请求量过大导致系统无法处理您的请求，您需要输入验证码才能继续后续的操作！'** 抓取达到限制后会网页上弹出此,已经对此做了休眠处理,并且发送邮件告警.
- **'您今天查看的简历数量已经超过限制，请明天继续查看!'** 当每天突破大概2W个抓取后,会无法查看简历,已经对此做了发送警告邮件,并且结束抓取,如果需要考虑提速的话,最合适的方式似乎以多进程抓取最为合适,单个程序提速,会遭到频繁要求输入验证码.

###  5.问题
- 依据工作年限+期望工作地进行抓取,从页面上的逻辑来观察,是不会抓取到9月份以前的,但是执行过程中发现抓取到不少5月份的
- 目前被移除的简历都是统一保存到了错误页面下


## 51job

###  1.运行说明
- ``` python main.py 51search /data/spider/cookie/cookie_id.txt /data/fetch/task/51/task_zone.txt ```
- 通过搜索获取最新的简历下载

 **log**

- [2015-08-22 14:15:55] 补充记录大概开始的 id 330367724
- [2015-08-22 14:15:02] 抓取到关键点发现命中率下降了
  具体时间点: [2015-08-22 13:36:50] 51-id search page 330434590 - 330434640 get 2 of 50, rate 4%
- [2015-08-22 14:16:32] 重新开始 320367724 开始抓, , 根据观察应该是 2014-12-08 左右录入的,
- [2015-09-06 10:18:21] 发现可能存在退出登录规则:  凌晨6点多左右, 会踢出不是当天(0点以后)登录的账号.

###  2.改进空间
- 依据当天更新的日期大约是45000左右,51job的抓取限制,每天大约抓取为20000以下,无法覆盖到当天更新的,需要考虑是否将条件缩小,跑两个进程进行抓取?
- 目前突出登录需要靠人工补救, 每天几乎每个账号至少一两次, 考虑自动登录
- 单账号有 2k/小时 左右的查看简历限额, 通过多账户可以提速
- 另外也可以把每个简历下面的"相似推荐"分析提取出来拉取(VIEWSTATE 不一定可用)
- 由于原来拼接id的方式抓取和现在是已搜索方式抓取,可能会存在相互冲突的简历,不考虑分开放,后续逻辑应该考虑,如果当前简历存在,检测更新时间和当前时间并将原来的覆盖掉,保持最新更新的

###  3.抓取策略[通过搜索页面-2015.11]
- 51job是动态页面,通过发送post数据进行不同页面的获取,依据期望工作地+工作年限进行页面跳转获取当前最近的更新,51job可以页面日期来进行回退到某一天更新的简历抓取,最大回退为6个月
- 51job搜索页面的隐藏关键字来改变页面请求在post中以下当中:
    + 技能关键字: ctrlSerach%24KEYWORD
    + 期望工作地关键字:hidWhere=......%7C030200%7C0%7C0%7C0000%7C99%23%25BeginPage%25%23%25EndPage%25%23[030200代表广州,040000代表深圳,010000代表北京,020000代表上海],替换此数字就可以实现地区之间跳转
    + 工作年限关键字:hidWhere=00%230%230%230%7C99%7C20150518%7C20151118%7C99%7C99%7C6%7C6%7C99%[7C6%7C6]此为工作年限关键字,修改数字改变对应页面工作年限]
    + 页面跳转关键字:pagerBottom%24txtGO=40 ,须加上才生效:  pagerBottom%24lbtnGO=+&[此参数在网页上点击选择数字跳转到其他搜索页面生成]
    + 日期跳转关键字:hidWhere=00%230%230%230%7C99%7C ***20150516*** %7C201 可以选择在6个月的时间内跳转到某一天来进行抓取
- 之前对于最近两天更新的日期判断是用BeautifulSoup选出第一个出现的更新日期,后续发现存在导入数据库表的问题导致,每个页面的第一个更新日期都是当天,新修改为将日期筛选出来加入到一个列表,并且依据列表最后一个日期进行判断是否大于2天
- 通过post data请求来获取简历搜索页面,会存在一些post data不可用的情况,正确的获取方法是从ehire.51job.com点进去,选择一个居住地,工作年限,学历,作为条件搜索,并且从最下面的框中随机跳到某一个页面,用此post data作为初始的post data来转发请求.由于post data固定的情况会存在请求到总数为3000错误页面(依据细分下来条件分割抓取,一个组合的搜索条件出现3000页面是不正常),这种错误与post data中__viewstat有关,突破这种错误的方法是,每抓取一页,用解析来获取__viewstat字段重新构造post data并且作为下次请求的post data

###  4.常见问题

- [2015-09-06 10:18:47] 可能存在的规律: 凌晨6点多左右, 会踢出不是当天(0点以后)登录的账号.
- 超时错误，程序会显示 ”一分钟超时次数过多，结束程序“，解决方案是验证网络访问以后，直接重启程序
- httplib.badstatus 错误，有段时间出现过，近段时间没怎么出现该错误，直接重启程序就好


###  5.多账户跑法

主要一下几个文件提前修改好:  cookie post last

```
 BEGIN_SRC shell
nohup python 51-id.py 30 > /tmp/51_30.log  2>&1 &
nohup python 51-id.py 31 > /tmp/51_31.log  2>&1 &
nohup python 51-id.py 32 > /tmp/51_32.log  2>&1 &


tail /data/spider/data/51/log/51-id.log -f
 END_SRC
```


###  6.其他

- 根据简历ID来搜索可能可行（不过我觉得id搜索命中率不高，20%的命中并不一定有）
- 行业和职位的搭配会造成大量的重复
- 搜索有限制，1个小时超过2000次简历抓取以后会屏蔽，抓取页面的时候要判断有没有这句话 ”系统显示：您的操作过于频繁，请注意劳逸结合，休息一会再来！“
- 51job子账号登录有数量限制问题，并且登录是后登录的会把前登录的踢下来
- 提速的方法应该是尽量避免重复，我觉得多账号其实现阶段作用不大（可能在id搜索的情况下适用，条件组合搜索下对提速并不明显）
- 有三个字段很特别，搜索的时候一定要注意，否则是无法搜出正确的简历的，ctrlSerach$hidSearchID，hidWhere，hidValue
- __VIEWSTATE这个表单字段很重要，需要从搜索页每一页提取下一页所需的值，否则只会搜出第一页
- 对于抓取51job简历的看法是用年龄分层，职位作条件，按性别2分查找即可，有一种可能是大类完全覆盖了小类，并且小类丢失了很多简历（这只是猜想）
- 通过地区+工作年限覆盖范围观察到某个时间点存在一个问题:有时候51job可能会大量的更新简历,60页是无法显示完全的,一般的情况下如果再条件上加上男\女组合来分割是可以的覆盖到的,但是如果大量更新还是没法完全覆盖.


## 中国人才网cjol

###  1.运行说明
- ```python main.py cjolsearch /data/spider/cookie/cjol.txt /data/fetch/task/cjolsearch/task_zone.txt ```
- 通过搜索获取最新的简历下载
- 现在 cjol 的全部简历更新都以 redis 为准， 再次出现的时间距 redis 里面的时间大于两天，就覆盖更新本地的简历

###  2.改进空间
- 依据当天更新的日期大约是6000左右,cjol的抓取限制,猜想是通过访问次数来使登录态失效，这个还需要观察
- 目前突出登录需要靠人工补救, cookie生存时间太短（大约两天）, 考虑自动登录
- 目前观察到爬虫cookie失效后，从浏览器的cookie还没有失效，浏览器的cookie会更新（目测是时间戳的更新），后续可以考虑自动更新爬虫cookie
- 由于原来拼接id的方式抓取和现在是已搜索方式抓取,可能会存在相互冲突的简历,因为cjol数据比较分散，所以暂时放于新文件夹中,后面将新简历覆盖旧简历即可
- 目前要探查cjol 更新的频率（白天跟夜晚）
- cjol 新版页面的解析

###  3.抓取策略[通过搜索页面-2015.11]
- 搜索页面的返回是通过post `http://newrms.cjol.com/SearchEngine/List?fn=d` 这个链接得到的
- 结果返回的是一个json，但是里面有一个键是 网页格式，需要的结果在里面
- cjol 观察，大概每两分钟，搜索结果页面就会更新，而且每次更新量不多
- post 数据为:
    + 'GetListResult':'GetListResult', # 应该返回的结果是列表形式
    + 'PageSize':40,  # 每一页的结果数
    + 'Sort':'UpdateTime desc', # 排序
    + 'PageNo':'1',  # 页数，通过更改这个达到翻页的目的

- 页面跳转，目前cjol 的页面跳转是通过post 来实现的，发现postdata相对固定，考虑到每次只有1000条简历，每页50，只有20页，所以把post 的数据写于一list 中，循环翻页

###  4.常见问题

- 可能存在的规律:通过访问次数的阈值踢出登录的账号.

###  6.其他

- 目前麻烦的是登录态的问题，解决了就可以不停跑了。而且登录态失效时间不固定，有时候2天，现在2月1号到5号，
登录态还没有失效。
- 旧版的搜索页面已经不更新搜索结果，目前是通过在新版搜索页面搜出来所有的更新的简历id，然后拼接旧版的简历
链接，抓下来的是旧版的简历页面（主要是考虑到新旧版的解析函数可能不同），旧版的简历页面的时间有待观察。


## liblogin 模块
### 1.说明
- 目前实现了51 的自动登陆
- 里面写了3 个网站的 登录态检查函数，这个可以做购买的时候的登陆态判断，但是没有作简历页面其他情况的判定
- 调用方式， `a = liblogin.Login51('company_name', 'username', 'password')  a.login() 或者 a.main() main会重试3次`

##  nginx正向代理请求转发
-  1.运行说明:爬虫持续抓取,会遭遇到封禁ip,临时启用代理做转发.
-  2.nginx正向代理服务端配置: '''    server {  
        resolver 8.8.8.8;                         #DNS解析地址
        resolver_timeout 5s;  
       
        listen 8080;                              #监听端口
       
        access_log  /home/reistlin/logs/proxy.access.log;  
        error_log   /home/reistlin/logs/proxy.error.log;  
       
        location / {  
            proxy_pass $scheme://$host$request_uri;  
            proxy_set_header Host $http_host;  
       
            proxy_buffers 256 4k;  
            proxy_max_temp_file_size 0;  
       
            proxy_connect_timeout 30;  
       
            proxy_cache_valid 200 302 10m;  
            proxy_cache_valid 301 1h;  
            proxy_cache_valid any 1m;  
    allow 127.0.0.1;                             #允许客户端访问ip
    deny all;  
    }  
    }   '''
-  3.nginx正向代理客户端配置:  '''export http_proxy = http://IP:port/'''  此为当前shell环境变量配置
     全局变量写入到 ''' vim /etc/profile ''' 
