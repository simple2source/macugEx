# 抓取系统

#### PS：本文档已经迁移至tech-doc：```https://git.oschina.net/hick/tech-doc/blob/master/python.md```

## TODO

- 进程的执行信息统一到一个文件中记录, 取代或者说替换现在的 `/data/fetch/51job_successtask_51_TASK_ID.txt` 的功能, 记录信息需要包括但不限于: 抓取的号段信息, 启动时间, 最后成功抓取的时间, 最近一段时间抓取的效率指标(根据具体情况看什么指标好计算, 比如最近1个小时抓取数, id命中率), 使用登录账户的信息(登录时间/账号名/是否已过期等)
- 需要提供 web 服务(http协议方式访问, 命令行方式提供, 由 init.php 来调用也可以), 接受简历来源和 id 参数, 可以随时抓取指定的 id 并返回简历 id/电话号码(如果有)
- 可以先知考虑单机, 但是以后可能要扩充到多机: 需要有任务池, 登录用户池, 整体的抓取速度/每个用户的抓取速度等多维度可以有封顶限制
- 运行python抓取进程的时候,存在多个进程对task文件任务数进行重复抓取,[解决设想] -- 检测生成对应的pid文件进行判定


优先级| 标题           | 详细说明
-----|----------------|----
3    | 目录结构优化    | TODO
2    | 运行监控        | TODO


## fetch 使用说明

### 1. 运行fetch抓取
- 抓取 51job /data/fetch 目录下执行 ```python main.py 51 /data/spider/cookie/[对应的cookie文件] /data/fetch/task/51/[对应的task文件]```

- 通过搜索页面抓取 cjol /data/fetch 目录下执行 ```python main.py cjolsearch /data/spider/cookie/cjolsearch.txt /data/fetch/task/cjolsearch/task_zone.txt```

- ```ps -aux | grep python``` 查看运行状态

- 抓取成功标志:  ```/data/fetch/sucess[对应任务名字]```  生成对应的文件,并且里面的数字有增长,数字代表当前抓取的条数


### 2. fetch系统组成

- ```/data/spider/cookie/[对应文件名]``` 通过helper动态抓取的帐号cookier实时同步到此文件,```ls -trl``` 查看cookie文件更新时间状态

- ```/data/fetch/task/51/[对应任务名]``` 自定义任务抓取id,格式如下:1,100;1,第一位代表初始范围id进行抓取;第二位代表到此id结束;第三位代表抓取系统从此id进行抓取一路遍历下去.

- ```/data/fetch/[sucess标识计数文件]``` 当生成文件,可以实时查看获取的当前已经抓取的条目数.

- ```/data/fetch/lib51search.py``` 通过加载本地文件,实现关键字搜索页面进行抓取,需要指定关键字作为文件加载

- ```/data/fetch/main.py``` 代码运行主文件

 - ```/data/fetch/libcjolsearch.py``` 通过加载cookie 文件，循环抓取搜索页面第一页

### 3. 问题分析

-  **1.问题** . 两个帐号spxx373, spxx336抓取不到文件,一直报错"最后没有抓取对应文件."
  +  ,经排查得出如下结论:由于抓取策略是依据id拼接成链接,并保存下来,链接方式如下: http://ehire.51job.com/Candidate/ResumeView.aspx?hidUserID=336240513 ,此两帐号经测试无法通过网页的方式来访问此页面,必须通过搜索并进行后面校验来获取页面,而其他帐号却可以通过这种网页id形式来直接访问页面,证明此2个帐号权限方面存在问题,另外还有其他几个帐号存在比较低的命中率,是否与权限有关,还需要进一步确定.

### 4. 搜索的方式抓取条件 更新:2016-1-12
-  通过搜索的方式抓取简历条件为:居住地+工作年限+年龄+性别+学历(部分过滤掉了大专以下学历),抓取的条件分的情况很细,容易出现很多搜索条件出来的页面是错误的,针对这种情况,对请求的错误页面做了判断并休眠5s避免触碰到抓取过于频繁.

hidWhere=00%230%230%230%7C99%7C20151231%7C20160107%7C31%7C31%7C7%7C7%7C99%7C000000%7C000000%7C99%7C99%7C99%7C0000%7C99%7C99%7C99%7C00%7C0000%7C99%7C99%7C99%7C0000%7C99%7C99%7C00%7C99%7C99%7C99%7C99%7C99%7C99%7C99%7C99%7C99%7C020000%7C0%7C0%7C0000%7C99%23%25BeginPage%25%23%25EndPage%25%23&hidSearchNameID=&hidEhireDemo=&hidNoSearch=&hidYellowTip=0 ***  其中要找到特定的post_data才能实现请求对应页面


### 5. 注意事项

- 抓取正常代码不多, 注意处理要严谨, 对比较可能的情况作出判断
  - 抓取到的网页根据具体情况有根据字符串判断正常抓取成功的标准, 保证抓取到的是数据, 而不是登录或者其他错误提示页面, 出错了一律记录 log 并保存出错页面
  - 当抓取账号出现数量限制等异常的时候, 终止抓取并发邮件出来

- Cookie 文件的处理，因为Unix系系统默认在文件末尾添加新行，而vi又不显示新行（Vim 7.4.785可以处理），有可能导致refer失效（cjol)，所以cookie文件的修改不要用vi。


##  智联

###  1.运行说明
- ``` python main.py zhilian /data/spider/cookie/cookie_id.txt /data/fetch/task/zhilian/task_zone.txt ```
- 通过搜索获取最新简历抓取,现在任务条件都直接写在程序当中,加载任务文件路径仅是为了传递一个参数并且用来生成记录抓取成功数目

###  2.改进空间

- 目前最难的, 不能按照 id 搜索, kelvin 有说根据公司名结合期望工作地等查, 意义不大, 跟关键词搜差不多
- 技术上可以把每个简历里推荐的也提炼出来, 但是基本只能推荐最近的, 比较老的应该抓不到
- 由于每天查看的数量是20000个,达到限制后会['您今天查看的简历数量已经超过限制，请明天继续查看!'],现在依据此判断发送告警邮件,并结束程序,由于目前cookie是写死路径的,需要考虑自动重新加载cookie的方法
- 以更新的频率来看,单个程序抓取的速度有效简历大概极限为2W-3W[一般可能在1W5],并且考虑帐号限制,单个进程难以覆盖到近2天更新的简历
- 以目前抓取的的情况来分析,单个进程的条件并不能完全覆盖大部分简历数,依据多个进程将不同的条件分割抓取,可以最大限度的抓取简历,抓取简历的条件应该优先考虑必要的条件,目前抓取的条件是依据task文件做判断,执行相应的条件抓取.
- 依据搜索来抓取每次一个条件最多显示67页,条件越多简历数目越多,年龄,行业是作为参数选项目前来看是一个比较好的条件,这个范围区间比较广,数目比较多

###  3.抓取策略
- 目前抓取的是按照工作年限+期望工作地进行组合获取最近更新的简历,由于每天更新的简历大概为45000个以上[以更新的最大频率来计算],此组合条件基本可以覆盖95%以上的简历更新.[后续经过验证,发现许多是重复更新的,实际上不同的简历是没有达到45000,大概是20000或以下,可能与具体日期有关]
- 目前依据页面的日期进行判断,抓取最近2天更新的简历.
- 抓取过程中发现以最大为2s间隔抓取到一定数量后会出现要求输入验证码,并且输入验证码后,另外过几分钟又会要求输入验证码,将此增大为3s后,暂时要求输入验证码频率立即下降很多,目前将输入验证码作为警报发出来,依据当前抓取的cookie,网页上提交验证码则可以继续抓取.
- 职位与行业搭配搜索效果并不好
- 按工作年限搜索并不准确，因为搜索一年到一年加两年到两年并不能覆盖1年到两年的结果，搜索的问题，但是要注意
- 按年龄或性别搜索非常准确
- 智联的登录态可以长期维持，几天都可以用
- 智联可以搜索3天内更新的简历，这对持续搜索很有帮助
- 现在还没有测试出限制频率，不过可以知道一个小时2000个简历是没有问题的
- 智联适合用条件组合来抓取简历，已经证实不合适用简历id来搜索简历
- 智联提速可能只有在初期，利用各类条件覆盖搜索来抓取可观的数据量，后期只要抓取最近更新的简历则好
- 每天抓取的最大简历数大概2万左右, 会显示 ”系统显示：您的操作过于频繁，请注意劳逸结合，休息一会再来！“，判断有没有这句话就好

###  4.常见错误

- 一般来说出现一些未知错误，但是都是可以直接启动进程解决的
- 曾出现过进程僵死的情况，很久都没抓取（现在只能ctrl c把进程干掉再启动，出现原因未明）
- 超时错误，参考51job的超时错误
- json decode error 直接重启进程就好
- httplib.badstatus 错误 ，参考51job的httplib.badstatus 错误
- 出现状态码503的错误，初步猜测抓取触碰了一些机制，可能是抓取数量，不过再重启进程就好了，不需要做特殊处理
- sqlite3.OperationalError: database is locked 写日记的时候加锁了，只能重启进程
- ValueError: No JSON object could be decoded 重启程序就好
- httplib.IncompleteRead: IncompleteRead (这个错误的出现 HTTP通道返回0字节，或者返回一定字节后卡住) ，重启进程
- **'因请求量过大导致系统无法处理您的请求，您需要输入验证码才能继续后续的操作！'** 抓取达到限制后会网页上弹出此,已经对此做了休眠处理,并且发送邮件告警.
- **'您今天查看的简历数量已经超过限制，请明天继续查看!'** 当每天突破大概2W个抓取后,会无法查看简历,已经对此做了发送警告邮件,并且结束抓取,如果需要考虑提速的话,最合适的方式似乎以多进程抓取最为合适,单个程序提速,会遭到频繁要求输入验证码.

###  5.问题
- 依据工作年限+期望工作地进行抓取,从页面上的逻辑来观察,是不会抓取到9月份以前的,但是执行过程中发现抓取到不少5月份的
- 目前被移除的简历都是统一保存到了错误页面下


## 51job

###  1.运行说明
- ``` python main.py 51search /data/spider/cookie/cookie_id.txt /data/fetch/task/51/task_zone.txt ```
- 通过搜索获取最新的简历下载

 **log**

- [2015-08-22 14:15:55] 补充记录大概开始的 id 330367724
- [2015-08-22 14:15:02] 抓取到关键点发现命中率下降了
  具体时间点: [2015-08-22 13:36:50] 51-id search page 330434590 - 330434640 get 2 of 50, rate 4%
- [2015-08-22 14:16:32] 重新开始 320367724 开始抓, , 根据观察应该是 2014-12-08 左右录入的,
- [2015-09-06 10:18:21] 发现可能存在退出登录规则:  凌晨6点多左右, 会踢出不是当天(0点以后)登录的账号.

###  2.改进空间
- 依据当天更新的日期大约是45000左右,51job的抓取限制,每天大约抓取为20000以下,无法覆盖到当天更新的,需要考虑是否将条件缩小,跑两个进程进行抓取?
- 目前突出登录需要靠人工补救, 每天几乎每个账号至少一两次, 考虑自动登录
- 单账号有 2k/小时 左右的查看简历限额, 通过多账户可以提速
- 另外也可以把每个简历下面的"相似推荐"分析提取出来拉取(VIEWSTATE 不一定可用)
- 由于原来拼接id的方式抓取和现在是已搜索方式抓取,可能会存在相互冲突的简历,不考虑分开放,后续逻辑应该考虑,如果当前简历存在,检测更新时间和当前时间并将原来的覆盖掉,保持最新更新的

###  3.抓取策略[通过搜索页面-2015.11]
- 51job是动态页面,通过发送post数据进行不同页面的获取,依据期望工作地+工作年限进行页面跳转获取当前最近的更新,51job可以页面日期来进行回退到某一天更新的简历抓取,最大回退为6个月
- 51job搜索页面的隐藏关键字来改变页面请求在post中以下当中:
    + 技能关键字: ctrlSerach%24KEYWORD
    + 期望工作地关键字:hidWhere=......%7C030200%7C0%7C0%7C0000%7C99%23%25BeginPage%25%23%25EndPage%25%23[030200代表广州,040000代表深圳,010000代表北京,020000代表上海],替换此数字就可以实现地区之间跳转
    + 工作年限关键字:hidWhere=00%230%230%230%7C99%7C20150518%7C20151118%7C99%7C99%7C6%7C6%7C99%[7C6%7C6]此为工作年限关键字,修改数字改变对应页面工作年限]
    + 页面跳转关键字:pagerBottom%24txtGO=40 ,须加上才生效:  pagerBottom%24lbtnGO=+&[此参数在网页上点击选择数字跳转到其他搜索页面生成]
    + 日期跳转关键字:hidWhere=00%230%230%230%7C99%7C ***20150516*** %7C201 可以选择在6个月的时间内跳转到某一天来进行抓取
- 之前对于最近两天更新的日期判断是用BeautifulSoup选出第一个出现的更新日期,后续发现存在导入数据库表的问题导致,每个页面的第一个更新日期都是当天,新修改为将日期筛选出来加入到一个列表,并且依据列表最后一个日期进行判断是否大于2天
- 通过post data请求来获取简历搜索页面,会存在一些post data不可用的情况,正确的获取方法是从ehire.51job.com点进去,选择一个居住地,工作年限,学历,作为条件搜索,并且从最下面的框中随机跳到某一个页面,用此post data作为初始的post data来转发请求.由于post data固定的情况会存在请求到总数为3000错误页面(依据细分下来条件分割抓取,一个组合的搜索条件出现3000页面是不正常),这种错误与post data中__viewstat有关,突破这种错误的方法是,每抓取一页,用解析来获取__viewstat字段重新构造post data并且作为下次请求的post data

###  4.常见问题

- [2015-09-06 10:18:47] 可能存在的规律: 凌晨6点多左右, 会踢出不是当天(0点以后)登录的账号.
- 超时错误，程序会显示 ”一分钟超时次数过多，结束程序“，解决方案是验证网络访问以后，直接重启程序
- httplib.badstatus 错误，有段时间出现过，近段时间没怎么出现该错误，直接重启程序就好


###  5.多账户跑法

主要一下几个文件提前修改好:  cookie post last

```
 BEGIN_SRC shell
nohup python 51-id.py 30 > /tmp/51_30.log  2>&1 &
nohup python 51-id.py 31 > /tmp/51_31.log  2>&1 &
nohup python 51-id.py 32 > /tmp/51_32.log  2>&1 &


tail /data/spider/data/51/log/51-id.log -f
 END_SRC
```


###  6.其他

- 根据简历ID来搜索可能可行（不过我觉得id搜索命中率不高，20%的命中并不一定有）
- 行业和职位的搭配会造成大量的重复
- 搜索有限制，1个小时超过2000次简历抓取以后会屏蔽，抓取页面的时候要判断有没有这句话 ”系统显示：您的操作过于频繁，请注意劳逸结合，休息一会再来！“
- 51job子账号登录有数量限制问题，并且登录是后登录的会把前登录的踢下来
- 提速的方法应该是尽量避免重复，我觉得多账号其实现阶段作用不大（可能在id搜索的情况下适用，条件组合搜索下对提速并不明显）
- 有三个字段很特别，搜索的时候一定要注意，否则是无法搜出正确的简历的，ctrlSerach$hidSearchID，hidWhere，hidValue
- __VIEWSTATE这个表单字段很重要，需要从搜索页每一页提取下一页所需的值，否则只会搜出第一页
- 对于抓取51job简历的看法是用年龄分层，职位作条件，按性别2分查找即可，有一种可能是大类完全覆盖了小类，并且小类丢失了很多简历（这只是猜想）
- 通过地区+工作年限覆盖范围观察到某个时间点存在一个问题:有时候51job可能会大量的更新简历,60页是无法显示完全的,一般的情况下如果再条件上加上男\女组合来分割是可以的覆盖到的,但是如果大量更新还是没法完全覆盖.


## 中国人才网cjol

###  1.运行说明
- ```python main.py cjolsearch /data/spider/cookie/cjol.txt /data/fetch/task/cjolsearch/task_zone.txt ```
- 通过搜索获取最新的简历下载
- 现在 cjol 的全部简历更新都以 redis 为准， 再次出现的时间距 redis 里面的时间大于两天，就覆盖更新本地的简历

###  2.改进空间
- 依据当天更新的日期大约是6000左右,cjol的抓取限制,猜想是通过访问次数来使登录态失效，这个还需要观察
- 目前突出登录需要靠人工补救, cookie生存时间太短（大约两天）, 考虑自动登录
- 目前观察到爬虫cookie失效后，从浏览器的cookie还没有失效，浏览器的cookie会更新（目测是时间戳的更新），后续可以考虑自动更新爬虫cookie
- 由于原来拼接id的方式抓取和现在是已搜索方式抓取,可能会存在相互冲突的简历,因为cjol数据比较分散，所以暂时放于新文件夹中,后面将新简历覆盖旧简历即可
- 目前要探查cjol 更新的频率（白天跟夜晚）
- cjol 新版页面的解析

###  3.抓取策略[通过搜索页面-2015.11]
- 搜索页面的返回是通过post `http://newrms.cjol.com/SearchEngine/List?fn=d` 这个链接得到的
- 结果返回的是一个json，但是里面有一个键是 网页格式，需要的结果在里面
- cjol 观察，大概每两分钟，搜索结果页面就会更新，而且每次更新量不多
- post 数据为:
    + 'GetListResult':'GetListResult', # 应该返回的结果是列表形式
    + 'PageSize':40,  # 每一页的结果数
    + 'Sort':'UpdateTime desc', # 排序
    + 'PageNo':'1',  # 页数，通过更改这个达到翻页的目的

- 页面跳转，目前cjol 的页面跳转是通过post 来实现的，发现postdata相对固定，考虑到每次只有1000条简历，每页50，只有20页，所以把post 的数据写于一list 中，循环翻页

###  4.常见问题

- 经观察， 报警邮件后的失效 cookie 还是可以继续抓取，故采用了2个账号，减少抓取频率，一个星期了，cookie 还没有过期

###  6.其他

- 旧版的搜索页面已经不更新搜索结果，目前是通过在新版搜索页面搜出来所有的更新的简历id，然后拼接旧版的简历
链接，抓下来的是旧版的简历页面（主要是考虑到新旧版的解析函数可能不同），旧版的简历页面的时间有待观察。

------------

## liblogin 模块
### 1.说明
- 目前实现了51 的自动登陆
- 里面写了3 个网站的 登录态检查函数，这个可以做购买的时候的登陆态判断，但是没有作简历页面其他情况的判定
- 调用方式， `a = liblogin.Login51('company_name', 'username', 'password')  a.login() 或者 a.main() main会重试3次`

##  nginx正向代理请求转发
-  1.运行说明:爬虫持续抓取,会遭遇到封禁ip,临时启用代理做转发.
-  2.nginx正向代理服务端配置:
          server {  
        resolver 8.8.8.8;                         #DNS解析地址
        resolver_timeout 5s;  

        listen 8080;                              #监听端口

        access_log  /home/reistlin/logs/proxy.access.log;  
        error_log   /home/reistlin/logs/proxy.error.log;  

        location / {  
            proxy_pass $scheme://$host$request_uri;  
            proxy_set_header Host $http_host;  

            proxy_buffers 256 4k;  
            proxy_max_temp_file_size 0;  

            proxy_connect_timeout 30;  

            proxy_cache_valid 200 302 10m;  
            proxy_cache_valid 301 1h;  
            proxy_cache_valid any 1m;  
        allow 127.0.0.1;                             #允许客户端访问ip
        deny all;  
        }  
        }
-  3.nginx正向代理客户端配置:  '''export http_proxy = http://IP:port/'''  此为当前shell环境变量配置
     全局变量写入到 ''' vim /etc/profile '''

------

## 账号切换

- 51, zhilian, cjol 均启用了 从 redis 中读取 cookie, redis 中 存储结构为 hash 结构

  + 51的key 为 cookie51_ 加用户名
  + zhilian 的key 为 cookiezl_ 加用户名
  + cjol 的key 为 cookiecjol_ 加用户名

- hash 结构为 ck: cookie字符串; mo: 修改时间; ex: 过期时间

- 输入脚本 `python input_ck.py` 会自动处理 redis hash结构

- 51job 的cookie 过期后会自动登陆， cjol， zhilian 的目前是没过期

- 有效抓取30 个后切换cookie ， 挑选的是 400秒抓取少于150个 的cookie



  --------

## 搜狗微信抓取相关

### 流程

1. 访问 http://weixin.sogou.com/ 公众号的具体微信号， 如 `LinuxHub` 访问链接 http://weixin.sogou.com/weixin?type=1&query=LinuxHub&ie=utf8

2. 在返回的页面里拿到与公众号ID相符和的链接，主要是微信的 openid(每个公众号唯一)， ext(临时生成，跟cookie有关)，

3. 这两个后面要用来请求一个类json的页面，得到的是一些临时链接（跟cookie 绑定，非请求的cookie 会返回请求过期），然后得到 mp.weixin.qq.com 微信文章页，解析，保存


### 防抓策略

1. http://weixin.sogou.com 对单IP 来源的请求有限制，假如一定时间内，访问次数过多，会导致一段时间内，不带cookie（具体可以用浏览器的隐身模式）访问的话，必须输入验证码，输入验证码后的cookie 可以继续抓取。

2. 单cookie 的抓取也会遇到限制，表现为 会302 到 一个url包含 aitispider 的页面输入验证码， 可以用多个cookie 绕过

3. cookie 有过期时间，上星期的cookie 可以正常搜索，但是到最后一步跳转到 微信文章页的时候会提示 “请求过期”


--------

## 脉脉相关

### app 抓包

1. 经测试，Android x86 虚拟机在本地电脑跑的速度过慢，而且代理的设置有问题，故采用手机方式

2. 采用的是fiddler 抓包的方式，注意在 设置的 `fiddler option` - `Connection` - `Allow remote computer to connect ` 里面打上勾，才能局域网里面抓包。

3. 手机通过WiFi 连接，与fiddler 电脑在同一局域网里面，然后在WiFi 设置里面，使用代理打上勾， 代理地址为，局域网内fiddler 所在电脑的IP

4. 连上WiFi 之后，因为 脉脉采用的是 https 方式连接，故还需安装伪证书，手机浏览器访问fiddler 的代理地址， http://ip:8888  点击 `FiddlerRoot certificate`
安装证书。

5. 手机登陆脉脉，观察 fiddler ，找到需要信息，done。

### 脉脉二度人脉post 信息 `验证key长期有效`

+ 地址为 url = 'https://open.taou.com/maimai/contact/v3/feed?'

    data1 = {   # 能翻100多页
            'page':	1,
            'u': 259592,
            'access_token':	'2.00IOgcnBc9xsMBf9c9031477sUNAQC',
            'version': '4.5.8',
            '_csrf': 'ErP9OU5H-_ruMV2rxprThRb5as8zvBl8D-dU',
            'channel': 'XiaoMi',
            'dist':	2,
            }

--------

## flask web 框架
-  flask 是一个微型的web框架，基于python的werkzeug,jinja2模版引擎，简单轻型可拓展。

### flask 安装
- ```pip install Flask```

### flask 基础使用
 
```
      from flask import Flask
      app = Flask(__name__)
      @app.route("/")
      def hello():
          return "<h1>hello world!</h1>" 
       if __name__ == "__main__":
            app.run()
```

- 运行flask ```python hello.py ``` ,默认运行在5000端口上，监听本机127.0.0.1

- 更多用法参考：http://dormousehole.readthedocs.org/en/latest/quickstart.html#id7

### flask url参数传递
- 视图函数获取get方法传入url参数,通过request得到具体传入值

```
        from flask import request
        http://123.58.128.216:8086/cjol/?page=2&area=%E6%B7%B1%E5%9C%B3&degree=60&keyword=ios

        @app.route('/cjol/'):
        def cjol():
            page = request.args.get('page')
            ...
            ...
            return page
```

---
### 当前123.58.128.213 上flask服务应用

#### 透传相关服务51job、智联、cjol

- **透传服务加密信息更新**
    + 透传服务，从网页中有大量加密串信息需要提取，51job网页接口需要网页需要获取vwstat状态信息作为postdata参数.入库接口信息51job和智联当前都需要从网页中获取id对应的加密串信息作为url进行传递.
    + 51job网页接口 ```/job51/```、id入库接口```/id51/```,以此类推，如下文所示.
    + 更新加密串信息策略，当前是使用Redisupdateinfo这个程序持续对目录下 ```/data/fetch/db/trans/``` 文件变化进行监控。每次访问接口后，将获取到的 html保存到 ```/data/fetch/db/trans/```。当对应的文件进行变化的时候，提取里面的加密串信息更新redis。
    + 当访问id接口的时候，从redis获取加密串信息,然后请求对应目标网站（51job或者智联），返回json给前端
  
- **透传服务http get参数:**
       
        ```
        参数              说明                                        备注
        area            居住地区                            当前支持：上海、北京、广州、深圳
        year            工作年限
        age             年龄
        degree          学历
        keyword         搜索关键字
        keywordtype     搜索关键字类型（指代51job搜索页面上展示的全文、职务、公司、证书、工作，传递参数依次是：0、2、3、4、5、1） [仅51job有此参数]
        page            页数
        industry        行业
        work_func       职能
        job_area        期望工作地
        updatetime      更新时间
        ```

- 51job搜索服务与直接id入库服务，访问示例:
  + 搜索服务: ```http://123.58.128.216:8086/job51/?parameter=xxx&...```  
  + 搜索关键字参数传递示例:
       
         ```
    http://123.58.128.216:8086/job51/?area=北京&degree=本科&year=3-4年&age=27&industry=计算机软件
    直接传递以下列出的部分参数,其余可传递参数参考代码
    area    传递参数: area=北京,其他可传递参数 ：上海、广州、深圳、7C000000【代表所有地区】
    year    传递参数: year=在读学生,其他可传递参数：应届毕业生、一年以下、1-2年、2-3年、3-4年、5-7年、8-9年、10年以上、7C99【代表所工作年龄段】
    degree  传递参数:degree=初中,其他可传递参数：高中、中技、中专、大专以下、大专、本科、硕士、博士、7C99【代表所有学历】
    industry传递参数:industry=计算机软件

        ```
    
  + id入库服务: ```http://123.58.128.216:8086/id51/?id=xxxx...```
- 智联搜索服务与直接id入库服务,访问示例:
  + 搜索服务:```http://123.58.128.216:8086/zhilian/?parameter=xxx&...```
  + 搜索关键字参数传递示例:
        
        ```
    http://123.58.128.216:8086/zhilian/?page=1&area=上海&degree=7%2C7&keyword=php&year=3%2C99   
    直接传递以下列出的部分参数,其余可传递参数参考代码
    area          传递参数:area=北京, 其他可传递参数：上海、广州、深圳
    industry      传递参数:industry=计算机软件,其他可传递参数：互联网服务、电子技术、通信、网络游戏、电信运营
    work_func     传递参数:work_func=软件工程师,其他可传递参数：软件开发、互联网产品、IT运维、通信技术、手机软件开发
    age           传递参数:age=21%2C31,代表21-31岁年龄段，中间%2C代表从多少到多少，urllib.unqote后是','符号
    degree        传递参数:degree=4%2C9,代表中专硕士学历，4代表中专，9代表硕士，【1-16】之间取值。
    year          传递参数:year=2%2C7,代表2-7年工作经验之间，2代表2年工作经验，7代表7年工作经验，以此类推
    updatetime    传递参数:updatetime=1%2C9，代表最近三天，其他具体可以参考代码
    
        ```
  + id入库服务:```http://123.58.128.216:8086/idz/?id=xxxx...```
- cjol搜索服务与直接Id入库服务,访问示例:
  + 搜索服务:``http://123.58.128.216:8086/cjol/?parameter=xxx...```
  + 搜索关键字参数传递示例:

        ```
    http://123.58.128.216:8086/cjol/?page=3&area=深圳&degree=60&keyword=ios
    直接传递以下列出的部分参数,其余可传递参数参考代码
    area            传递参数:area=北京,其他可传递参数：上海、广州、深圳
    degree          传递参数:degree=50,其他可传递参数：60、70、40、80 【参数对应的含义，具体参考代码】
    
        ```
  + id入库服务:```http://123.58.128.216:8086/idc/?id=xxx...```

#### worktitle接口服务
- 直接访问```http://123.58.128.216:8086/task/```web界面获取任务信息
    + ```https://api.worktile.com/``` 调用worktitle开放的API，并请求授权acess_token，用python urllib访问获取当前的任务信息
    + 实时抓取worktitle接口的json，生成html作为返回 
    + 每天对抓取的的worktitle对技术团队每个人的任务进度进行统计,并邮件通知
    + 每5分钟运行一次获取任务信息，并与上次获取的任务进行比较，将新增的任务和以完成的任务保存到本地文件，作为统计每天的个人任务进度.
    + 每天23：00将当天获取的任务信息记录（当天新增的任务和完成的任务）邮件通知，并清除当天的记录.
   
- 直接访问```http://123.58.128.216:8086/task/?form=json```获取返回任务信息的json接口


#### 关键词维护web界面
- **关键词web界面功能实现增删改查，分为以下4个接口,查询、增加、删除、修改**
  + 接口通用异常返回
        ```
        {
            "code":1,
            "msg":error # 根据异常状况返回相应信息
        }
        ```

- **查询接口/query/**
  + URL:```http://123.58.128.216:8086/hierarchy/query/```
  + 示例1:```http://123.58.128.216:8086/hierarchy/query/?father=php```
  + 示例2：```http://123.58.128.216:8086/hierarchy/query/?father=php&page=2```
  + HTTP get参数:
        ```
        参数        必选        说明
        father      false     父类查询
        son         false     子类查询   
    
        备注：进行搜索查询时father 和 son 关键字参数二选一,不能同时进行查询.
        ```
  + 返回数据
        ```
        {
            "code": 0,
            "msg": ok,
            "data":[[18, "PHP", 0, 13, 0.2],          #字段值依次为id(表结构主键),keyword,job_id,parent_id,score
                [143, "\u7f16\u8f91\u5668", 0, 18, 0.4]]
       
            }
        ```

- **插入接口/insert/**
  + URL:```http://123.58.128.216:8086/hierarchy/insert/```
  + 示例:```http://123.58.128.216:8086/hierarchy/insert/?keyword=1&job_id=1&parent_id=1&score=5```
  + HTTP get参数:
        ```
        参数       
        keyword
        job_id
        parent_id
        score
       
        备注：传递参数说明参见查询接口
        ```
  + 返回数据
        ```
        {
            "code":0,
            "msg":ok
            
            }
        ```

- **删除数据接口/del/**
  + URL:```http://123.58.128.216:8086/hierarchy/del/```
  + 示例:```http://123.58.128.216:8086/hierarchy/del/?primary_id=id```
  + HTTP get参数:
    
        ```
        参数            说明
        primary_id     主键id
        ```
  + 返回数据
    
        ```
        {
            "code":0,
            "msg":ok
            
            }
        ```

- **更数据新接口/update/**
  + URL:```http://123.58.128.216:8086/hierarchy/update/```
  + 示例:```http://123.58.128.216:8086/hierarchy/update/?primary_id=id```
  + HTTP get参数:
    
        ```
        参数            说明
        primary_id     主键id
        ```
  + 返回数据
    
        ```
        {
            "code":0,
            "msg":ok
            
            }
        ```
 - **接口访问翻页**
   + URL:```http://123.58.128.216:8086/hierarchyView```
   + 示例:```http://123.58.128.216:8086/hierarchyView?page=6```
   + HTTP get参数:
        ```
        参数         页数
        page         int
        ```
   + 返回数据
        ```
        {
            "code":0,
            "msg":ok,
            "data":[879, "java", 0, 1, 1.0]
        }


